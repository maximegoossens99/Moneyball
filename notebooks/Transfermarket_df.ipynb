{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f749914",
   "metadata": {},
   "source": [
    "# dataframe with transfermarket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf597d",
   "metadata": {},
   "source": [
    "## importing clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277c3a00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipynb in /Users/sacremukoko/.pyenv/versions/3.10.6/envs/Moneyball/lib/python3.10/site-packages (0.5.1)\n",
      "Requirement already satisfied: aiohttp in /Users/sacremukoko/.pyenv/versions/3.10.6/envs/Moneyball/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sacremukoko/.pyenv/versions/3.10.6/envs/Moneyball/lib/python3.10/site-packages (from aiohttp) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sacremukoko/.pyenv/versions/3.10.6/envs/Moneyball/lib/python3.10/site-packages (from aiohttp) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sacremukoko/.pyenv/versions/3.10.6/envs/Moneyball/lib/python3.10/site-packages (from aiohttp) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sacremukoko/.pyenv/versions/3.10.6/envs/Moneyball/lib/python3.10/site-packages (from aiohttp) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sacremukoko/.pyenv/versions/3.10.6/envs/Moneyball/lib/python3.10/site-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/sacremukoko/.pyenv/versions/3.10.6/envs/Moneyball/lib/python3.10/site-packages (from aiohttp) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/sacremukoko/.pyenv/versions/3.10.6/envs/Moneyball/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipynb\n",
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201ca9d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from aiohttp.client import ClientSession\n",
    "import nest_asyncio\n",
    "import string\n",
    "import re\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ae2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../raw_data/clean_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c417167",
   "metadata": {},
   "source": [
    "### Korean translation didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54293153",
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_df = df[df.nat == 'KOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_df.name[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1879a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fcf41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_korean(english_names):\n",
    "    translator = Translator()\n",
    "    korean_names = [translator.translate(name, dest='ko').text for name in english_names.replace(\"-\",\" \")]\n",
    "    return pd.Series(korean_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_names_series = translate_to_korean(kor_df.name[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_names_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62291e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_korean(name):\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(name, dest='ko')\n",
    "    korean_name = translation.text\n",
    "    return korean_name\n",
    " Example usage:\n",
    "english_name = 'Son Heung-Min'\n",
    "korean_name = translate_to_korean(english_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbcdb04",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb7a456",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "%%time\n",
    "async def fetch_and_process(session, url, name):\n",
    "    async with session.get(url, headers=headers) as response:\n",
    "        content = await response.text()\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        def player_link(): \n",
    "            if soup.find(class_=\"hauptlink\"):\n",
    "                return \"https://www.transfermarkt.com\" + soup.find(class_=\"hauptlink\").a.get('href')\n",
    "            return \"player not found\"\n",
    "            \n",
    "        return name, player_link()\n",
    "\n",
    "async def main(names_url):\n",
    "    my_conn = aiohttp.TCPConnector(limit=10)\n",
    "    \n",
    "    async with ClientSession(connector=my_conn) as session:\n",
    "        tasks = [fetch_and_process(session, url, name) for name, url in zip(names, names_url)]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Process the results, which are tuples of (name, player_link)\n",
    "    player_link = {name: link for name, link in results}\n",
    "    return player_link\n",
    "    \n",
    "    # Process the results, which are tuples of (name, player_link)\n",
    "    player_link = {name: link for name, link in results}\n",
    "    await player_link\n",
    "\n",
    "player_link = asyncio.run(main(names_url[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159a25c",
   "metadata": {},
   "source": [
    "## our base URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f68be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.transfermarkt.com/schnellsuche/ergebnis/schnellsuche?query=\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3559aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.Series(df.name)[:8750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37428fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.75 ms, sys: 3.41 ms, total: 12.2 ms\n",
      "Wall time: 11.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       https://www.transfermarkt.com/schnellsuche/erg...\n",
       "1       https://www.transfermarkt.com/schnellsuche/erg...\n",
       "2       https://www.transfermarkt.com/schnellsuche/erg...\n",
       "3       https://www.transfermarkt.com/schnellsuche/erg...\n",
       "4       https://www.transfermarkt.com/schnellsuche/erg...\n",
       "                              ...                        \n",
       "8745    https://www.transfermarkt.com/schnellsuche/erg...\n",
       "8746    https://www.transfermarkt.com/schnellsuche/erg...\n",
       "8747    https://www.transfermarkt.com/schnellsuche/erg...\n",
       "8748    https://www.transfermarkt.com/schnellsuche/erg...\n",
       "8749    https://www.transfermarkt.com/schnellsuche/erg...\n",
       "Name: name, Length: 8750, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "names_url = names.map(lambda x: url + x.lower().replace(\" \", \"+\"))\n",
    "names_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b2e0f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### first function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665723e1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "%%time\n",
    "async def fetch(session, url, headers):\n",
    "    async with session.get(url, headers=headers) as response:\n",
    "        return await response.text()\n",
    "\n",
    "async def scrape_page(url, headers, club_selector, image_selector):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        html = await fetch(session, url, headers)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extracting club\n",
    "        club_element = soup.select_one(club_selector)\n",
    "        club = club_element['alt'] if club_element else None\n",
    "\n",
    "        # Extracting profile image\n",
    "        image_element = soup.select_one(image_selector)\n",
    "        profile_image = image_element['src'] if image_element else None\n",
    "\n",
    "        return {'url': url, 'club': club, 'profile_image': profile_image}\n",
    "\n",
    "async def main(names_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    }\n",
    "\n",
    "    # Specify the CSS selectors for the club and image elements\n",
    "    club_selector = 'a[title^=\"\"] img.tiny_wappen'\n",
    "    image_selector = 'img.bilderrahmen-fixed'\n",
    "\n",
    "    # Use asyncio.gather to concurrently scrape multiple pages\n",
    "    tasks = [scrape_page(url, headers, club_selector, image_selector) for url in names_url]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example Pandas Series with URLs\n",
    "    names_url = names_url\n",
    "\n",
    "    asyncio.run(main(names_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57540f90",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### second function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54adac60",
   "metadata": {
    "hidden": true
   },
   "source": [
    "async def fetch(session, url, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, headers=headers) as response:\n",
    "            return await response.text()\n",
    "\n",
    "async def scrape_page(url, headers, club_selector, image_selector, semaphore):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        html = await fetch(session, url, headers, semaphore)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extracting club\n",
    "        club_element = soup.select_one(club_selector)\n",
    "        club = club_element['alt'] if club_element else 'None'\n",
    "\n",
    "        # Extracting profile image\n",
    "        image_element = soup.select_one(image_selector)\n",
    "        profile_image = image_element['src'] if image_element else 'None'\n",
    "\n",
    "        return {'url': url, 'club': club, 'profile_image': profile_image}\n",
    "\n",
    "async def main(names_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    }\n",
    "\n",
    "    # Specify the CSS selectors for the club and image elements\n",
    "    club_selector = 'a[title^=\"\"] img.tiny_wappen'\n",
    "    image_selector = 'img.bilderrahmen-fixed'\n",
    "\n",
    "    # Set the maximum number of concurrent requests\n",
    "    max_concurrent_requests = 4000\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "\n",
    "    # Use asyncio.gather to concurrently scrape multiple pages\n",
    "    tasks = [scrape_page(url, headers, club_selector, image_selector, semaphore) for url in names_url]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example Pandas Series with URLs\n",
    "    names_url = names_url\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    result_df = asyncio.run(main(names_url))\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Total time taken: {end_time - start_time} seconds\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3252a379",
   "metadata": {
    "hidden": true
   },
   "source": [
    "result_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b3001",
   "metadata": {
    "hidden": true
   },
   "source": [
    "result_df[result_df.club == 'None']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e9c90b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "result_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538f9cb",
   "metadata": {},
   "source": [
    "### second function upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb11fe75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def fetch(session, url, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, headers=headers) as response:\n",
    "            return await response.text()\n",
    "\n",
    "async def scrape_page(url, headers, club_selector, image_selector, year_selector, market_value_selector, name_selector, semaphore):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        html = await fetch(session, url, headers, semaphore)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extracting club\n",
    "        club_element = soup.select_one(club_selector)\n",
    "        club = club_element['alt'] if club_element else 'None'\n",
    "\n",
    "        # Extracting profile image\n",
    "        image_element = soup.select_one(image_selector)\n",
    "        profile_image = image_element['src'] if image_element else 'None'\n",
    "\n",
    "        # Extracting year\n",
    "        year_element = soup.select_one(year_selector)\n",
    "        year = year_element.text.strip() if year_element else 'None'\n",
    "\n",
    "        # Extracting market value\n",
    "        market_value_element = soup.select_one(market_value_selector)\n",
    "        market_value = market_value_element.text.strip() if market_value_element else 'None'\n",
    "        \n",
    "        # Extracting name\n",
    "        name_element = soup.select_one(name_selector)\n",
    "        name = name_element.text.strip() if name_element else 'None'\n",
    "\n",
    "        return {'name': name, 'club': club, 'profile_image': profile_image, 'year': year, 'market_value': market_value}\n",
    "\n",
    "async def main(names_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    }\n",
    "\n",
    "    # Specify the CSS selectors for the club, image, year, and market value elements\n",
    "    club_selector = 'a[title^=\"\"] img.tiny_wappen'\n",
    "    image_selector = 'img.bilderrahmen-fixed'\n",
    "    year_selector = 'td.zentriert:nth-of-type(4)'  # Update with the correct selector for the year\n",
    "    market_value_selector = 'td.rechts.hauptlink'  # Update with the correct selector for market value\n",
    "    name_selector = 'td.hauptlink a'\n",
    "\n",
    "    # Set the maximum number of concurrent requests\n",
    "    max_concurrent_requests = 3000\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "\n",
    "    # Use asyncio.gather to concurrently scrape multiple pages\n",
    "    tasks = [scrape_page(url, headers, club_selector, image_selector, year_selector, market_value_selector, name_selector, semaphore) for url in names_url]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example Pandas Series with URLs\n",
    "    names_url = names_url\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    result_df = asyncio.run(main(names_url))\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Total time taken: {end_time - start_time} seconds\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58449bb6",
   "metadata": {},
   "source": [
    "### new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "858e640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "async def fetch(session, url, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.get(url, headers=headers) as response:\n",
    "                return await response.text()\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "async def scrape_page(url, headers, club_selector, image_selector, year_selector, market_value_selector, name_selector, semaphore):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        html = await fetch(session, url, headers, semaphore)\n",
    "        if html is None:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extracting club\n",
    "        club_element = soup.select_one(club_selector)\n",
    "        club = club_element['alt'] if club_element else 'None'\n",
    "\n",
    "        # Extracting profile image\n",
    "        image_element = soup.select_one(image_selector)\n",
    "        profile_image = image_element['src'] if image_element else 'None'\n",
    "\n",
    "        # Extracting year\n",
    "        year_element = soup.select_one(year_selector)\n",
    "        year = year_element.text.strip() if year_element else 'None'\n",
    "\n",
    "        # Extracting market value\n",
    "        market_value_element = soup.select_one(market_value_selector)\n",
    "        market_value = market_value_element.text.strip() if market_value_element else 'None'\n",
    "\n",
    "        # Extracting name\n",
    "        name_element = soup.select_one(name_selector)\n",
    "        name = name_element.text.strip() if name_element else 'None'\n",
    "\n",
    "        return {'name': name, 'club': club, 'profile_image': profile_image, 'year': year, 'market_value': market_value}\n",
    "\n",
    "async def main(names_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    }\n",
    "\n",
    "    # Specify the CSS selectors for the club, image, year, and market value elements\n",
    "    club_selector = 'a[title^=\"\"] img.tiny_wappen'\n",
    "    image_selector = 'img.bilderrahmen-fixed'\n",
    "    year_selector = 'td.zentriert:nth-of-type(4)'\n",
    "    market_value_selector = 'td.rechts.hauptlink.vereinsfarben'\n",
    "    name_selector = 'td.hauptlink a.spielprofil_tooltip'\n",
    "\n",
    "    # Set the maximum number of concurrent requests\n",
    "    max_concurrent_requests = 3000\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "\n",
    "    # Use asyncio.gather to concurrently scrape multiple pages\n",
    "    tasks = [scrape_page(url, headers, club_selector, image_selector, year_selector, market_value_selector, name_selector, semaphore) for url in names_url]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example Pandas Series with URLs\n",
    "    names_url = names_url\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    result_df = asyncio.run(main(names_url))\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Total time taken: {end_time - start_time} seconds\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98bbc501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17c2bba0",
   "metadata": {},
   "source": [
    "### scrape club logos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4279577",
   "metadata": {},
   "outputs": [],
   "source": [
    "clubs = pd.Series(df.club.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "clubs_url = clubs.map(lambda x: url + x.lower().replace(\" \", \"+\"))\n",
    "clubs_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc6253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def scrape_page(url, headers, club_logo_selector, club_name_selector, semaphore):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        html = await fetch(session, url, headers, semaphore)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extracting club logo\n",
    "        club_logo_element = soup.select_one(club_logo_selector)\n",
    "        club_logo = club_logo_element['src'] if club_logo_element else 'None'\n",
    "\n",
    "        # Extracting club name\n",
    "        club_name_element = soup.select_one(club_name_selector)\n",
    "        club_name = club_name_element['alt'].strip() if club_name_element and 'alt' in club_name_element.attrs else 'None'\n",
    "\n",
    "        return {'club_name': club_name, 'club_logo': club_logo}\n",
    "\n",
    "async def main(names_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    }\n",
    "\n",
    "    # Specify the CSS selectors for the club logo and club name elements\n",
    "    club_logo_selector = 'td.zentriert.suche-vereinswappen img'\n",
    "    club_name_selector = 'td.zentriert.suche-vereinswappen img'\n",
    "\n",
    "\n",
    "    # Set the maximum number of concurrent requests\n",
    "    max_concurrent_requests = 4000\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "\n",
    "    # Use asyncio.gather to concurrently scrape multiple pages\n",
    "    tasks = [scrape_page(url, headers, club_logo_selector, club_name_selector, semaphore) for url in names_url]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example Pandas Series with URLs\n",
    "    names_url = clubs_url\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    result_df = asyncio.run(main(names_url))\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Total time taken: {end_time - start_time} seconds\")\n",
    "result_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\"fm_name\"] = clubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99851ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df.club_name == 'None']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3384c56d",
   "metadata": {},
   "source": [
    "## rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e83d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def common_names(dictio):\n",
    "    num = 0\n",
    "    result = []\n",
    "    for x in dictio.keys():\n",
    "        if not \" \" in x:\n",
    "            result.append(x)\n",
    "            num += 1\n",
    "        result = result\n",
    "    print(num)\n",
    "    return result\n",
    "check_names = common_names(player_link)\n",
    "check_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_chars = string.ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7b5dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def name_filter(name):\n",
    "    for x in name:\n",
    "        if x in allowed_chars:\n",
    "            continue\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_player_info(player_info):\n",
    "    cleaned_info = []\n",
    "    \n",
    "    if player_info is str:\n",
    "        return player_info\n",
    "    else:\n",
    "\n",
    "        # Extract name\n",
    "        cleaned_info.append(player_info[0])\n",
    "        \n",
    "\n",
    "        # Extract age\n",
    "        try:\n",
    "            if len(player_info) < 11:\n",
    "                cleaned_info.append(player_info[2])\n",
    "            cleaned_info.append(player_info[3])\n",
    "        except: \n",
    "            return cleaned_info\n",
    "            \n",
    "\n",
    "        '''# Extract height\n",
    "        try:\n",
    "            cleaned_info.append(player_info[4].replace(\"\\xa0m\", \"\").replace(\",\", \".\"))\n",
    "        except:\n",
    "            return cleaned_info\n",
    "        # Extract position\n",
    "        try:\n",
    "            cleaned_info.append(player_info[6].strip())\n",
    "        except:\n",
    "            cleaned_info.append(player_info[-1].strip())'''\n",
    "\n",
    "    return cleaned_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9621aa",
   "metadata": {},
   "source": [
    "I have some issues with cleaning the data of Dutch players and probably some other countries but I didn't notice them yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3baa9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def process(session, url):\n",
    "    try:\n",
    "        async with session.get(url, headers=headers) as response:\n",
    "            response.raise_for_status()\n",
    "            content = await response.text()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Extract the full name if found\n",
    "            full_name = \"Full Name not found\"\n",
    "            info_content = soup.find_all(class_=\"info-table__content info-table__content--bold\")\n",
    "            if info_content:\n",
    "                full_name = [info.text for info in info_content]\n",
    "                \n",
    "\n",
    "            return full_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "async def info(player_links):\n",
    "    my_conn = aiohttp.TCPConnector(limit=10)\n",
    "\n",
    "    async with ClientSession(connector=my_conn) as session:\n",
    "        # Create a list of tasks for processing player information\n",
    "        tasks = [process(session, url) for url in player_links.values()]\n",
    "\n",
    "        # Execute the tasks concurrently and collect the results\n",
    "        full_names = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Combine player names with the keys from the original player_links dictionary\n",
    "    player_info = {name: full_name for name, full_name in zip(player_links.keys(), full_names)}\n",
    "    player_info = {name: ([name] + full_name  if \", \" in full_name[0] else full_name) for name, full_name in player_info.items()}\n",
    "    #player_info = {name: clean_player_info(full_name) for name, full_name in player_info.items()}\n",
    "    \n",
    "    \n",
    "\n",
    "    return player_info\n",
    "\n",
    "\n",
    "\n",
    "full_name = asyncio.run(info(player_link))\n",
    "full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24335ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(full_name['Jaden Slory']))\n",
    "full_name['Jaden Slory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c568c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(full_name['Maximilian Beier']))\n",
    "full_name['Maximilian Beier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e067f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(full_name['Eliesse Ben Seghir']))\n",
    "full_name['Eliesse Ben Seghir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511390d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(full_name['Pedri']))\n",
    "full_name['Pedri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61ea44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_name['Joris Chotard']\n",
    " 'Ayman Kari': 'Ayman Kari',\n",
    " 'Mohamed El Arouch': 'Mohamed El Arouch',\n",
    " 'Malo Gusto': 'Malo Gusto',\n",
    " 'Lucas Gourna-Douath': 'Lucas Gourna-Douath',\n",
    " 'Koba Lein': 'Full Name not found',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496dc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [2,3,4,5]\n",
    "[1] + a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef094e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'PedroGonzálezLópez'.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837197f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_player_info(player_info):\n",
    "    cleaned_info = []\n",
    "\n",
    "    # Extract name\n",
    "    cleaned_info.append(player_info[0])\n",
    "        \n",
    "\n",
    "    # Extract age\n",
    "    cleaned_info.append(player_info[3])\n",
    "\n",
    "    # Extract height\n",
    "    cleaned_info.append(player_info[4].replace(\"\\xa0m\", \"\").replace(\",\", \".\"))\n",
    "\n",
    "    # Extract position\n",
    "    cleaned_info.append(player_info[6].strip())\n",
    "\n",
    "    return cleaned_info\n",
    "\n",
    "clean_player_info(full_name['Pedri'])\n",
    "#clean_player_info(full_name['Joris Chotard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e466d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_name['Pedri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a851f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_filter(link, name):\n",
    "    if name in check_names:\n",
    "        if f'/{name.lower()}/' in link:\n",
    "            return link\n",
    "        return 'confusing_name'\n",
    "    return link\n",
    "player_link = {key: link_filter(value, key) for key, value in player_link.items()}\n",
    "player_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_nones(dictio):\n",
    "    result = 0\n",
    "    for x,y in dictio.items():\n",
    "        if y == 'player not found':\n",
    "            result += 1\n",
    "            print(x)\n",
    "        result = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d84e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c46bc1b",
   "metadata": {},
   "source": [
    "✅ for common_names\n",
    "if /common_name/ is not in link\n",
    "return confusing_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d5ab4",
   "metadata": {},
   "source": [
    "from every transfermarket link\n",
    "\n",
    "we will retrieve:\n",
    "\n",
    "✅ the full name --> will help us for the link on sorare\n",
    "\n",
    "current club --> FM is outdated\n",
    "\n",
    "current market value\n",
    "\n",
    "position\n",
    "\n",
    "age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1babd12b",
   "metadata": {},
   "source": [
    "---\n",
    "make a dictionary with the available leagues on sorare\n",
    "\n",
    "create two tables with:\n",
    "\n",
    "available vs non_available on sorare\n",
    "\n",
    "filter the available on injury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70dce86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae656a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
